{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "USE_COLAB = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34c9a8f53eb0256f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c30cf5e0fa8be4ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f084ab2f7cf453d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_samples(dataset, split, seed):\n",
    "    global class_to_id\n",
    "    np.random.seed(seed)\n",
    "    print(f\"Plot Samples (seed = {seed}, {split} split)\")\n",
    "    \n",
    "    llb = list(class_to_id.keys())\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    found_samples = {i: False for i in range(8)}\n",
    "    \n",
    "    while not all(found_samples.values()):\n",
    "        idx = np.random.randint(0, len(dataset))\n",
    "        image , label = dataset[idx]\n",
    "        if found_samples[label]:\n",
    "            continue\n",
    "        \n",
    "        plt.subplot(2, 4, label + 1)\n",
    "        plt.imshow(image.squeeze().permute(1, 2, 0))\n",
    "        plt.title(f'{label}. {llb[label]}')\n",
    "        plt.axis('off')\n",
    "        found_samples[label] = True\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "303d4052356bf102"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_label_distribution_histogram(dataset, split):\n",
    "    if type(dataset) == torch.utils.data.dataset.ConcatDataset:\n",
    "        labels = []\n",
    "        for d in dataset.datasets:\n",
    "            labels += d.targets\n",
    "    elif type(dataset) in [Subset, list]:\n",
    "        labels = [label for _, label in dataset]\n",
    "    else:\n",
    "        labels = dataset.targets\n",
    "        \n",
    "    labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(8), counts)\n",
    "\n",
    "    for i in range(8):\n",
    "        ax.text(i, counts[i], f\"{100*(counts[i]/sum(counts)):.1f} %\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    ax.set_xlabel(\"Label\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xticks(range(8))\n",
    "    ax.set_xticklabels(range(8), ha=\"center\")\n",
    "    ax.set_title(f\"Histogram of labels distribution for {split} split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89fbbcc57315fb35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and Prepare datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a577cb20ac5aac5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    TRAIN_DATA_PATH = \"/content/drive/MyDrive/DLNN/dataset/TRAIN\"\n",
    "    TUNE_DATA_PATH = \"/content/drive/MyDrive/DLNN/dataset/TUNE\"\n",
    "else:\n",
    "    TRAIN_DATA_PATH = \"dataset/TRAIN\"\n",
    "    TUNE_DATA_PATH = \"dataset/TUNE\"\n",
    "TRAIN_SPLIT_OF_ALL = 0.8"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74b547f4bb6454d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scale = torchvision.transforms.Lambda(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "transforms_augment = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=20), \n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    scale,\n",
    "])\n",
    "transforms_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    scale,\n",
    "])\n",
    "\n",
    "train_valid_aug_dataset = ImageFolder(\n",
    "    root=TRAIN_DATA_PATH,\n",
    "    transform=transforms_augment,\n",
    ")\n",
    "train_valid_raw_dataset = ImageFolder(\n",
    "    root=TRAIN_DATA_PATH,\n",
    "    transform=transforms_norm,\n",
    ")\n",
    "train_valid_dataset = torch.utils.data.ConcatDataset([train_valid_raw_dataset, train_valid_aug_dataset])\n",
    "tune_dataset = ImageFolder(\n",
    "    root=TUNE_DATA_PATH,\n",
    "    transform=transforms_norm,\n",
    ")\n",
    "\n",
    "class_to_id = tune_dataset.class_to_idx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35db714c29e7a79f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explore datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93d27958beb9629d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_samples(\n",
    "    dataset=train_valid_dataset, \n",
    "    split=\"train+valid\", \n",
    "    seed=21,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c99c31cbf2723c86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_label_distribution_histogram(\n",
    "    dataset=train_valid_dataset, \n",
    "    split=\"train+valid\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d03b2d787b4973b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_label_distribution_histogram(\n",
    "    dataset=tune_dataset, \n",
    "    split=\"tune\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8df13ff927c3c584"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split train-validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3880ec6aacda31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = []\n",
    "for d in train_valid_dataset.datasets:\n",
    "    labels += d.targets\n",
    "\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    np.arange(len(train_valid_dataset)),\n",
    "    test_size=1-TRAIN_SPLIT_OF_ALL,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(train_valid_dataset, train_indices)\n",
    "valid_dataset = Subset(train_valid_dataset, valid_indices)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e7c50f2c9562c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_label_distribution_histogram(\n",
    "    dataset=train_dataset, \n",
    "    split=\"train\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5f88750aa9ed65e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_label_distribution_histogram(\n",
    "    dataset=valid_dataset, \n",
    "    split=\"valid\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8d31cc7470d419"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a97e6fdc575003"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fa17108405e534c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train-Eval helpers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e93bd7e430b394"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_eval(model, loader, loss_function=nn.CrossEntropyLoss(), device=\"auto\"):\n",
    "    \"\"\"Returns test_loss, test_accuracy\"\"\"\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    sample_count = 0\n",
    "    if device == \"auto\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    itr = tqdm(loader, total=len(loader), leave=False)\n",
    "\n",
    "    for inputs, labels in itr:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct += torch.sum(preds == labels).item()\n",
    "        sample_count += np.shape(inputs)[0]\n",
    "\n",
    "        itr.set_description(\"(Eval)\")\n",
    "        itr.set_postfix(loss=round(loss.item(), 5), accuracy=round(test_correct/sample_count, 2))\n",
    "\n",
    "    test_loss = test_loss / len(loader)\n",
    "    test_acc = test_correct / sample_count\n",
    "\n",
    "    return test_loss, test_acc\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71317942f0d7b44c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        batch_size,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        train_set,\n",
    "        test_set=None,\n",
    "        device=\"auto\",\n",
    "):\n",
    "    \"\"\"\n",
    "    If test_set is not None, it would evaluate model for each epoch,\n",
    "    train_set would be in  N*(img, label) format.\n",
    "    \"\"\"\n",
    "    if device == \"auto\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    needs_test_evaluation = test_set is not None\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "\n",
    "    test_loss_list = None if test_set is None else []\n",
    "    test_acc_list = None if test_set is None else []\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    if needs_test_evaluation:\n",
    "        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        samples_count = 0\n",
    "        itr = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "\n",
    "        for inputs, labels in itr:\n",
    "            samples_count += np.shape(inputs)[0]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            itr.set_description(f\"(Training) Epoch [{epoch + 1}/{epochs}]\")\n",
    "            itr.set_postfix(loss=round(loss.item(), 5), accuracy=round(train_correct/samples_count, 3))\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / len(train_set)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "\n",
    "        if needs_test_evaluation:\n",
    "            test_loss, test_acc = model_eval(\n",
    "                model=model,\n",
    "                device=device,\n",
    "                loader=test_loader,\n",
    "            )\n",
    "\n",
    "            test_loss_list.append(test_loss)\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "    history = {\n",
    "        \"train loss\": train_loss_list,\n",
    "        \"train acc\": train_acc_list,\n",
    "    }\n",
    "    if needs_test_evaluation:\n",
    "        history.update({\n",
    "            \"test loss\": test_loss_list,\n",
    "            \"test acc\": test_acc_list,\n",
    "        })\n",
    "\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c835be2a5c43322"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b161060dff081ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trend_plot_helper(pobj):\n",
    "    plt.figure(figsize=(5*len(pobj), 5))\n",
    "    for idx, (titler, plots) in enumerate(pobj.items(), start=1):\n",
    "        plt.subplot(1, len(pobj), idx)\n",
    "        for label, data in plots:\n",
    "            plt.plot(range(1, len(data)+1), data, label=label)\n",
    "        yt, xt = titler.split(' - ')\n",
    "        plt.xlabel(xt)\n",
    "        plt.ylabel(yt)\n",
    "        plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c15dd07d99809ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_roc_plot(model, dataset, device='auto'):\n",
    "    global class_to_id\n",
    "    llb = list(class_to_id.keys())\n",
    "    if device == 'auto':\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    probabilities = []\n",
    "    labels = []\n",
    "    \n",
    "    for data, label in tqdm(loader, leave=False, desc=\"Generate data\"):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        prob = model(data)\n",
    "        \n",
    "        probabilities.append(prob.detach())\n",
    "        labels.append(label.detach())\n",
    "        \n",
    "    probabilities = torch.cat(probabilities, dim=0).cpu()\n",
    "    labels = torch.cat(labels, dim=0).cpu()\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    auc = {}\n",
    "    for i in tqdm(range(8), leave=False, desc=\"Calculate ROC/AUC\"):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(labels == i, probabilities[:, i])\n",
    "        auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--', label=\"Chance Level (AUC = 0.5)\")\n",
    "    for i in tqdm(range(8), leave=False, desc=\"Generate plot\"):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'[{i}] Class {llb[i]} (auc: {auc[i]:.3f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1bdee5db6cb6198"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_confusion_matrix(model, dataset, device='auto'):\n",
    "    global class_to_id\n",
    "    llb = list(class_to_id.keys())\n",
    "    if device == 'auto':\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    probabilities = []\n",
    "    labels = []\n",
    "    \n",
    "    for data, label in tqdm(loader, leave=False, desc=\"Generate data\"):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        prob = model(data)\n",
    "        \n",
    "        probabilities.append(prob.detach())\n",
    "        labels.append(label.detach())\n",
    "    probabilities = torch.cat(probabilities, dim=0)\n",
    "    _, predicted = torch.max(probabilities.detach(), dim=1)\n",
    "    labels = torch.cat(labels, dim=0).detach().cpu().squeeze().numpy()\n",
    "    predicted = predicted.detach().cpu().squeeze().numpy()\n",
    "    \n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=labels,\n",
    "        y_pred=predicted,\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm, cmap='Blues', annot=True, cbar=False, fmt=\".0f\",\n",
    "                xticklabels=llb, yticklabels=llb)\n",
    "    plt.xlabel('Predicted Label', labelpad=20)\n",
    "    plt.ylabel('True Label', labelpad=20)\n",
    "    plt.title('Confusion Matrix', fontsize=30)\n",
    "    \n",
    "    recall = metrics.recall_score(y_true=labels, y_pred=predicted, average='macro')\n",
    "    f1 = metrics.f1_score(y_true=labels, y_pred=predicted, average='macro')\n",
    "    precision = metrics.precision_score(y_true=labels, y_pred=predicted, average='macro')\n",
    "    report = metrics.classification_report(y_true=labels, y_pred=predicted)\n",
    "\n",
    "    return {'recall': recall, 'f1': f1, 'precision': precision, 'report': report}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d317df0ae8c58b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Costume conv block"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d0e6159744e9321"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, kernel=(3,3), stride=(1,1), padding=\"same\"):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_chan, out_channels=out_chan, kernel_size=kernel, padding=padding, stride=stride),\n",
    "            nn.BatchNorm2d(out_chan),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b447b938792034f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AlexNet model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f79732b8cdaa9ab2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            ConvBlock(in_chan=in_channel, out_chan=16, kernel=(9,9)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=16, out_chan=32, kernel=(7,7)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=32, out_chan=64, kernel=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=64, out_chan=128, kernel=(3, 3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=128, out_chan=128, kernel=(3,3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(in_features=2048, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=num_classes),\n",
    "        )\n",
    "        \n",
    "        self.av_net = nn.Linear(num_classes, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.network(x))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd16f65e6f6da0e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "59ed5b4bdffc42dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VGG model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8936d94044e864d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            ConvBlock(in_chan=in_channel, out_chan=16, kernel=(3,3)),\n",
    "            ConvBlock(in_chan=16, out_chan=16, kernel=(3,3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=16, out_chan=32, kernel=(3,3)),\n",
    "            ConvBlock(in_chan=32, out_chan=32, kernel=(3,3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=32, out_chan=64, kernel=(3,3)),\n",
    "            ConvBlock(in_chan=64, out_chan=64, kernel=(3,3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=64, out_chan=128, kernel=(3, 3)),\n",
    "            ConvBlock(in_chan=128, out_chan=128, kernel=(3, 3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            ConvBlock(in_chan=128, out_chan=128, kernel=(3,3)),\n",
    "            ConvBlock(in_chan=128, out_chan=128, kernel=(3,3)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(in_features=2048, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=num_classes),\n",
    "        )\n",
    "        \n",
    "        self.av_net = nn.Linear(num_classes, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.network(x))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39a6d76d259a215a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MobileNet model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa114e33599704b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DepthWiseSeparableConv(nn.Module):\n",
    "    def __init__(self, inc, outc, stride=1):\n",
    "        super(DepthWiseSeparableConv, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(inc, inc, kernel_size=3, padding=1, groups=inc), # the depth-wise part\n",
    "            nn.Conv2d(inc, outc, kernel_size=1, stride=stride), # the point-wise part\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13ca0e0cd76a876"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes):\n",
    "        super(MobileNet, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            ConvBlock(in_chan=in_channel, out_chan=32, kernel=(3,3), stride=(2,2), padding=1),\n",
    "            DepthWiseSeparableConv(inc=32, outc=64, stride=1),\n",
    "            DepthWiseSeparableConv(inc=64, outc=128, stride=2),          \n",
    "            DepthWiseSeparableConv(inc=128, outc=128, stride=1),\n",
    "            DepthWiseSeparableConv(inc=128, outc=256, stride=2),\n",
    "            DepthWiseSeparableConv(inc=256, outc=256, stride=1),\n",
    "            DepthWiseSeparableConv(inc=256, outc=512, stride=2),          \n",
    "            DepthWiseSeparableConv(inc=512, outc=512, stride=1),\n",
    "            DepthWiseSeparableConv(inc=512, outc=512, stride=1),\n",
    "            DepthWiseSeparableConv(inc=512, outc=512, stride=1),\n",
    "            DepthWiseSeparableConv(inc=512, outc=512, stride=1),\n",
    "            DepthWiseSeparableConv(inc=512, outc=512, stride=1),\n",
    "            DepthWiseSeparableConv(inc=512, outc=1024, stride=2),\n",
    "            DepthWiseSeparableConv(inc=1024, outc=1024, stride=1),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=1024, out_features=num_classes),\n",
    "        )\n",
    "        \n",
    "        self.av_net = nn.Linear(num_classes, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.network(x))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "290fb4a306c348a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and Tune models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2a0afa1c89c1c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AlexNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c05f7efc740cf31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alexnet_model = AlexNet(in_channel=3, num_classes=8)\n",
    "summary(alexnet_model, (3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a4de2ed0c69d07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alexnet_train_history = train_model(\n",
    "    model=alexnet_model,\n",
    "    batch_size=400,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(alexnet_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8),\n",
    "    epochs=24,\n",
    "    train_set=train_dataset,\n",
    "    test_set=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21916668860b9551"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", alexnet_train_history[\"train acc\"]),\n",
    "            (\"Validation Acc\", alexnet_train_history[\"test acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", alexnet_train_history[\"train loss\"]),\n",
    "            (\"Validation Loss\", alexnet_train_history[\"test loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "531f453680e3c3dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alexnet_tune_history = train_model(\n",
    "    model=alexnet_model,\n",
    "    batch_size=400,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(alexnet_model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2),\n",
    "    epochs=16,\n",
    "    train_set=tune_dataset,\n",
    "    test_set=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec030fe0111875b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Tune Acc\", alexnet_tune_history[\"train acc\"]),\n",
    "            (\"Validation Acc\", alexnet_tune_history[\"test acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Tune Loss\", alexnet_tune_history[\"train loss\"]),\n",
    "            (\"Validation Loss\", alexnet_tune_history[\"test loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c156c161df482b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_roc_plot(\n",
    "    model=alexnet_model,\n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8050c6ea84bca54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alexnet_report = generate_confusion_matrix(\n",
    "    model=alexnet_model, \n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d5da194814c1a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Recall:    {alexnet_report['recall']:.3f}\")\n",
    "print(f\"F1:        {alexnet_report['f1']:.3f}\")\n",
    "print(f\"Precision: {alexnet_report['precision']:.3f}\")\n",
    "print(alexnet_report['report'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e069f1476be0ccc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del alexnet_model, alexnet_report, alexnet_tune_history, alexnet_train_history\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c752cda66cbf41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VGG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "494b9d5ee27769c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vgg_model = VGG(in_channel=3, num_classes=8)\n",
    "summary(vgg_model, (3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df92a40e20c2241d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vgg_train_history = train_model(\n",
    "    model=vgg_model,\n",
    "    batch_size=400,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(vgg_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8),\n",
    "    epochs=24,\n",
    "    train_set=train_dataset,\n",
    "    test_set=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caee95407e49c97d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", vgg_train_history[\"train acc\"]),\n",
    "            (\"Validation Acc\", vgg_train_history[\"test acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", vgg_train_history[\"train loss\"]),\n",
    "            (\"Validation Loss\", vgg_train_history[\"test loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8089f51758238ba1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vgg_tune_history = train_model(\n",
    "    model=vgg_model,\n",
    "    batch_size=400,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(vgg_model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2),\n",
    "    epochs=16,\n",
    "    train_set=tune_dataset,\n",
    "    test_set=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f4b0e116a73660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Tune Acc\", vgg_tune_history[\"train acc\"]),\n",
    "            (\"Validation Acc\", vgg_tune_history[\"test acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Tune Loss\", vgg_tune_history[\"train loss\"]),\n",
    "            (\"Validation Loss\", vgg_tune_history[\"test loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ca6b6f70dba521b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_roc_plot(\n",
    "    model=vgg_model,\n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa14d0daa17ad68a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vgg_report = generate_confusion_matrix(\n",
    "    model=vgg_model,\n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79fb0477f0160731"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Recall:    {vgg_report['recall']:.3f}\")\n",
    "print(f\"F1:        {vgg_report['f1']:.3f}\")\n",
    "print(f\"Precision: {vgg_report['precision']:.3f}\")\n",
    "print(vgg_report['report'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb881d5e0cef1d12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del vgg_model, vgg_report, vgg_tune_history, vgg_train_history\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f433d9a867fe1387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MobileNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e66d07588ad3debd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mobilenet_model = MobileNet(in_channel=3, num_classes=8)\n",
    "summary(mobilenet_model, (3, 128, 128))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d259aefc665ba36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mobilenet_train_history = train_model(\n",
    "    model=mobilenet_model,\n",
    "    batch_size=250,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(mobilenet_model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8),\n",
    "    epochs=24,\n",
    "    train_set=train_dataset,\n",
    "    test_set=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e340a70676ff6954"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", mobilenet_train_history[\"train acc\"]),\n",
    "            (\"Validation Acc\", mobilenet_train_history[\"test acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", mobilenet_train_history[\"train loss\"]),\n",
    "            (\"Validation Loss\", mobilenet_train_history[\"test loss\"])\n",
    "        ]\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "866c4bee4cedbc48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mobilenet_tune_history = train_model(\n",
    "    model=mobilenet_model,\n",
    "    batch_size=400,\n",
    "    loss_function=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(mobilenet_model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2),\n",
    "    epochs=16,\n",
    "    train_set=tune_dataset,\n",
    "    test_set=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c18d403de49b129"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Tune Acc\", mobilenet_tune_history[\"train acc\"]),\n",
    "            (\"Validation Acc\", mobilenet_tune_history[\"test acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Tune Loss\", mobilenet_tune_history[\"train loss\"]),\n",
    "            (\"Validation Loss\", mobilenet_tune_history[\"test loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91c19c200f68d308"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generate_roc_plot(\n",
    "    model=mobilenet_model,\n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a26baef4cce3bc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mobilenet_report = generate_confusion_matrix(\n",
    "    model=mobilenet_model,\n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a026760e13848f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Recall:    {mobilenet_report['recall']:.3f}\")\n",
    "print(f\"F1:        {mobilenet_report['f1']:.3f}\")\n",
    "print(f\"Precision: {mobilenet_report['precision']:.3f}\")\n",
    "print(mobilenet_report['report'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "945b98a649f8cb91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del mobilenet_model, mobilenet_report, mobilenet_tune_history, mobilenet_train_history\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6f6e71adac44e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
