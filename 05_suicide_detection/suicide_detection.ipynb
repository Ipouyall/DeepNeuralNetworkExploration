{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install gensim emoji nltk tqdm seaborn torch torchsummary -q"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec65989e04597f0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:06.972177Z",
     "start_time": "2023-12-19T20:02:04.797605Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import gensim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "MODE = \"local\" # valid modes are 'local', 'colab' and 'kaggle'\n",
    "SAMPLE_NUMBERS = 4\n",
    "SEQUENCE_LEN = 256\n",
    "TEST_PORTION = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:42.268701Z",
     "start_time": "2023-12-19T20:02:42.245154Z"
    }
   },
   "id": "7a284f33993328c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "if   MODE == \"local\":\n",
    "    BASE_PATH = \".\"\n",
    "elif MODE == \"colab\":\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Deep Neural Network - UT/CA 04\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(f\"Unknown mode {MODE}\")\n",
    "    exit(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:42.590766Z",
     "start_time": "2023-12-19T20:02:42.565364Z"
    }
   },
   "id": "183e3ae190990fe3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(BASE_PATH, \"twitter-suicidal_data.csv\")\n",
    "\n",
    "W2V_PATH = os.path.join(BASE_PATH, \"w2v_vectors.kv\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:42.818432Z",
     "start_time": "2023-12-19T20:02:42.809172Z"
    }
   },
   "id": "b1f939dd5898a0b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27110752c038c1d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b050729991fec9"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:43.880427Z",
     "start_time": "2023-12-19T20:02:43.840245Z"
    }
   },
   "id": "88e92f34d72c6f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data statistics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194e8bd73f51d23b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               tweet  intention\n0  my life is meaningless i just want to end my l...          1\n1  muttering i wanna die to myself daily for a fe...          1\n2  work slave i really feel like my only purpose ...          1\n3  i did something on the 2 of october i overdose...          1\n4  i feel like no one cares i just want to die ma...          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>intention</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>my life is meaningless i just want to end my l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>muttering i wanna die to myself daily for a fe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>work slave i really feel like my only purpose ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i did something on the 2 of october i overdose...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i feel like no one cares i just want to die ma...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:44.648841Z",
     "start_time": "2023-12-19T20:02:44.557975Z"
    }
   },
   "id": "eaa0e791280d469b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9119 entries, 0 to 9118\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet      9119 non-null   object\n",
      " 1   intention  9119 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 142.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:45.213811Z",
     "start_time": "2023-12-19T20:02:45.013250Z"
    }
   },
   "id": "9f8728ed33e76432"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be081ae5bbcd06a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intentions = df[\"intention\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=intentions.index, y=intentions.values)\n",
    "plt.title(\"Intentions Distribution\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8c06e8ae4c8406"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89035345 1.14044522]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "labels = df[\"intention\"]\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "print(class_weights)\n",
    "del labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T20:02:47.913679Z",
     "start_time": "2023-12-19T20:02:47.870261Z"
    }
   },
   "id": "e9323acf7f7de0ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57f7552c58ea498e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "nltk.download([\"stopwords\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f16efd0184e90f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:          \n",
    "        return 'n'\n",
    "\n",
    "def preprocess_data(text: str):\n",
    "    \"\"\"\n",
    "    Preprocessing steps are as follows:\n",
    "    0. concatenation of the text (not necessary)\n",
    "    1. lowercase the text\n",
    "    2. remove punctuation\n",
    "    3. remove numbers\n",
    "    4. remove urls\n",
    "    5. remove usernames\n",
    "    6. remove extra spaces\n",
    "    7. convert emojis to text\n",
    "    8. remove non-word characters\n",
    "    9. lemmatization and tokenization of the text\n",
    "    10. remove stopwords\n",
    "    :param text: str\n",
    "    :return: tokens: list[str]\n",
    "    \"\"\"\n",
    "    # text = ''.join(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove urls,\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # remove usernames\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # convert emojis to text\n",
    "    text = convert_emoji_to_text(text)\n",
    "    # remove non-word characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # lemmatization and tokenization of the text\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    pos_tag = nltk.pos_tag(tokenized)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token, nltk_pos_tagger(tag))\n",
    "        for token, tag in pos_tag\n",
    "    ]\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe50f445aa09ecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_length_bound(text: str, length:int):\n",
    "    if len(text) <= length:\n",
    "        return text\n",
    "    mm = length // 2 - 3\n",
    "    rr = length - mm - 5\n",
    "    return text[:mm] + \"|...|\" + text[-rr:]\n",
    "    \n",
    "def draw_sample_processing(dataframe: pd.DataFrame, sample_numbers: int = SAMPLE_NUMBERS, show=True):\n",
    "    sample_text = dataframe.sample(sample_numbers)\n",
    "    label_rawT_procT = [\n",
    "        (label, tweet, preprocess_data(tweet)) \n",
    "        for tweet, label in zip(sample_text[\"tweet\"], sample_text[\"intention\"])\n",
    "    ]\n",
    "    if show:\n",
    "        for idx, (label, rawT, procT) in enumerate(label_rawT_procT, start=1):\n",
    "            procT = str(procT)\n",
    "            print(f\"Sample {idx}:\")\n",
    "            print(f\":::::::Label          : {label}\")\n",
    "            print(f\":::::::Raw Tweet      : {format_length_bound(rawT, 50):<50} ({len(rawT)})\")\n",
    "            print(f\":::::::Processed Tweet: {format_length_bound(procT, 50):<50} ({len(procT)})\")\n",
    "        return \n",
    "    return pd.DataFrame(label_rawT_procT, columns=[\"Intention\", \"Raw Tweet\", \"Processed Tweet\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ac7d394e03d9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_sample_processing(df, show=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a757363e6e6cbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec - Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e129c7a31663741"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print available word2vec models\n",
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d15b31459fd1e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(W2V_PATH):\n",
    "    print(\"Loading Word2Vec model...\")\n",
    "    w2v_model = gensim.models.KeyedVectors.load(W2V_PATH, mmap='r')\n",
    "else:\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Saving Word2Vec model...\")\n",
    "    w2v_model.save(W2V_PATH)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bbe6faa1226b8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_VECTOR_DIM = w2v_model.vector_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d53926889defc38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21730f8dd01e5b4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Twitter(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, w2v_model: gensim.models.KeyedVectors, aggregate=True, **kw_args):\n",
    "        self.dataframe = dataframe\n",
    "        self.w2v_model = w2v_model\n",
    "        self.aggregate = aggregate\n",
    "        self.max_sequence_len = kw_args.get(\"sequence_len\", 512)\n",
    "        if not aggregate and \"sequence_len\" not in kw_args:\n",
    "            print(\"You Should provide 'sequence_len' to use not aggregate option!\")\n",
    "            exit(1)\n",
    "\n",
    "        self.len = len(self.dataframe)\n",
    "        self.vector_size = w2v_model.vector_size\n",
    "        self.lazy_mode = kw_args.get(\"lazy\", True)\n",
    "\n",
    "        self._proc_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.lazy_mode:\n",
    "            vector = self._get_word_vectors(self.dataframe.iloc[idx][\"tokens\"])\n",
    "            return vector, self.dataframe.iloc[idx][\"intention\"]\n",
    "        return self.dataframe.iloc[idx][\"vector\"], self.dataframe.iloc[idx][\"intention\"]\n",
    "\n",
    "    def get_vector_size(self):\n",
    "        return self.vector_size\n",
    "\n",
    "    def _proc_dataset(self):\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tweet\"].map(preprocess_data)\n",
    "        \n",
    "        # delete samples with empty tokens\n",
    "        lwz = len(self.dataframe)\n",
    "        self.dataframe = self.dataframe[self.dataframe[\"tokens\"].map(len) > 0]\n",
    "        self.dataframe.reset_index(drop=True, inplace=True)\n",
    "        print(f\"Deleted 0-Len Samples: {lwz - len(self.dataframe)}\")\n",
    "        self.seq_report()\n",
    "\n",
    "        if not self.aggregate:\n",
    "            self.dataframe[\"tokens\"] = self.dataframe[\"tokens\"].map(self._pad)\n",
    "\n",
    "        if not self.lazy_mode:\n",
    "            self.dataframe[\"vector\"] = self.dataframe[\"tokens\"].map(self._get_word_vectors)\n",
    "\n",
    "    def _get_word_vectors(self, tokens: list) -> torch.tensor:\n",
    "        if not self.aggregate:\n",
    "            return torch.stack([\n",
    "                torch.tensor(self.w2v_model[token] if token in self.w2v_model else np.zeros(self.vector_size), dtype=torch.float32) \n",
    "                for token in tokens\n",
    "            ])\n",
    "        wv = np.zeros(self.vector_size)\n",
    "        vc = 0\n",
    "        for token in tokens:\n",
    "            if token in self.w2v_model:\n",
    "                wv += self.w2v_model[token]\n",
    "                vc += 1\n",
    "        wv = torch.tensor(wv / max(vc, 1), dtype=torch.float32)\n",
    "        return wv\n",
    "\n",
    "    def _pad(self, tokens: list):\n",
    "        if len(tokens) >= self.max_sequence_len:\n",
    "            return tokens[:self.max_sequence_len]\n",
    "        return tokens + [\" \"] * (self.max_sequence_len - len(tokens))\n",
    "\n",
    "    def seq_report(self):\n",
    "        length_all = self.dataframe[\"tokens\"].map(len).tolist()\n",
    "        max_length = np.max(length_all)\n",
    "        print(f\"Sequence Length Report\")\n",
    "        print(f\":::::MAX  LENGTH:::[{max_length:^5}]\")\n",
    "        print(f\":::::MIN  LENGTH:::[{np.min(length_all):^5}]\")\n",
    "        print(f\":::::MEAN LENGTH:::[{np.mean(length_all):^5}]\")\n",
    "\n",
    "        all_tokens = set()\n",
    "        for token_set in self.dataframe[\"tokens\"].tolist():\n",
    "            all_tokens = all_tokens.union(set(token_set))\n",
    "        unique_tokens_count = len(all_tokens)\n",
    "        valid_tokens = sum(1 if token in self.w2v_model else 0 for token in all_tokens)\n",
    "        print(\"Sequence Tokenization Report\")\n",
    "        print(f\":::::All Unique Tokens:::[{unique_tokens_count:^6}\")\n",
    "        print(f\":::::All Valid Tokens:::[{valid_tokens:^6}\")\n",
    "        print(f\":::::Valid Tokens:::[{round(100*valid_tokens/unique_tokens_count, 2):^5}%]\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tensor(tokens: list):\n",
    "        return torch.tensor(tokens, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e3f4dc9d30a97f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b87e7e8b267a1c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Data into train-valid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd00ab880892e646"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=TEST_PORTION, random_state=42, stratify=df[\"intention\"])\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a54facda2729872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "670fcd8db0e5651c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = Twitter(train_df, w2v_model, aggregate=False, sequence_len=SEQUENCE_LEN)\n",
    "valid_dataset = Twitter(valid_df, w2v_model, aggregate=False, sequence_len=SEQUENCE_LEN)\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset length: {len(valid_dataset)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e4a5b690afadad5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_dataset.seq_report()\n",
    "# print()\n",
    "# valid_dataset.seq_report()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1a37e4da1bd343"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd1da733e79676ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ec673024264c4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b851fd32cccfa14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa29012e257e3d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_eval(model, loader, loss_function, device):\n",
    "    \"\"\"Returns test_loss, test_acc\"\"\"\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    total_samples = 0\n",
    "    if device == \"auto\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    itr = tqdm(loader, total=len(loader), leave=False)\n",
    "\n",
    "    for inputs, labels in itr:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        sample_count = len(inputs)\n",
    "    \n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct = torch.sum(preds == labels).item()\n",
    "        test_acc += test_correct\n",
    "\n",
    "        itr.set_description(\"(Eval)\")\n",
    "        itr.set_postfix(\n",
    "            loss=round(loss.item(), 5),\n",
    "            accuracy=round(test_correct/sample_count, 5),\n",
    "            \n",
    "            )\n",
    "        total_samples += sample_count\n",
    "\n",
    "    test_loss = test_loss / len(loader)\n",
    "    test_acc = test_acc / total_samples\n",
    "\n",
    "    return test_loss, test_acc\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b33657ae501516"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        batch_size,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        train_set,\n",
    "        valid_set,\n",
    "        device,\n",
    "):\n",
    "    \n",
    "    if device == \"auto\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    itr = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_samples = 0\n",
    "        for idx, (inputs, labels) in enumerate(itr, start=1):\n",
    "            epoch_samples += len(inputs)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            epoch_acc += torch.sum(preds == labels).item()\n",
    "        \n",
    "            loss = loss_function(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            itr.set_description(f\"(Training) Epoch [{epoch + 1}/{epochs}]\")\n",
    "            itr.set_postfix(\n",
    "              loss=round(loss.item(), 5),\n",
    "              accuracy=round(epoch_acc/epoch_samples, 5),\n",
    "              )\n",
    "        \n",
    "        epoch_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc/epoch_samples)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss, valid_acc = model_eval(\n",
    "            model=model,\n",
    "            loader=valid_loader,\n",
    "            loss_function=loss_function,\n",
    "            device=device\n",
    "            )\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "    \n",
    "    history = {\n",
    "      \"train_loss\": train_losses,\n",
    "      \"train_acc\": train_accs,\n",
    "    \n",
    "      \"valid_loss\": valid_losses,\n",
    "      \"valid_acc\": valid_accs,\n",
    "    }\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bc1edee98f616b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trend_plot_helper(pobj):\n",
    "    plt.figure(figsize=(5*len(pobj), 5))\n",
    "    for idx, (titler, plots) in enumerate(pobj.items(), start=1):\n",
    "        plt.subplot(1, len(pobj), idx)\n",
    "        for label, trend in plots:\n",
    "            plt.plot(range(1, len(trend)+1), trend, label=label)\n",
    "        yt, xt = titler.split(' - ')\n",
    "        plt.xlabel(xt)\n",
    "        plt.ylabel(yt)\n",
    "        plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e5acc20a7d5d446"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bed208d730342fc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DetectorLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super(DetectorLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5a723d9c509e7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train modes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e073c31a727473a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train\n",
    "lstm_1l_model = DetectorLSTM(input_size=EMBEDDING_VECTOR_DIM, hidden_size=SEQUENCE_LEN, num_layers=1, num_classes=2)\n",
    "# summary(lstm_1l_model, (EMBEDDING_VECTOR_DIM, SEQUENCE_LEN))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82644f31350a2d35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_1l_model_train_history = train_model(\n",
    "    model=lstm_1l_model,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_function=nn.CrossEntropyLoss(weight=weights),\n",
    "    optimizer=torch.optim.Adam(lstm_1l_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    epochs=EPOCHS,\n",
    "    train_set=train_dataset,\n",
    "    valid_set=valid_dataset,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "205eeb10bcb61818"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", lstm_1l_model_train_history[\"train_acc\"]),\n",
    "            (\"Validation Acc\", lstm_1l_model_train_history[\"valid_acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", lstm_1l_model_train_history[\"train_loss\"]),\n",
    "            (\"Validation Loss\", lstm_1l_model_train_history[\"valid_loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71ccd8933f362f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del lstm_1l_model_train_history, lstm_1l_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b723a2160d4c00c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-Layer LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f9b51f8fe11d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_2l_model = DetectorLSTM(input_size=EMBEDDING_VECTOR_DIM, hidden_size=SEQUENCE_LEN, num_layers=2, num_classes=2)\n",
    "# summary(lstm_2l_model, (EMBEDDING_VECTOR_DIM, SEQUENCE_LEN))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38205ca41f01b782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_2l_model_train_history = train_model(\n",
    "    model=lstm_2l_model,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_function=nn.CrossEntropyLoss(weight=weights),\n",
    "    optimizer=torch.optim.Adam(lstm_2l_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    epochs=EPOCHS,\n",
    "    train_set=train_dataset,\n",
    "    valid_set=valid_dataset,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d12a8548a160858a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", lstm_2l_model_train_history[\"train_acc\"]),\n",
    "            (\"Validation Acc\", lstm_2l_model_train_history[\"valid_acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", lstm_2l_model_train_history[\"train_loss\"]),\n",
    "            (\"Validation Loss\", lstm_2l_model_train_history[\"valid_loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "949e5c8c8632cc0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del lstm_2l_model_train_history, lstm_2l_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4945fd541fe218c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN+2-Layer-LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1566b2a8428318c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DetectorCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super(DetectorCNNLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ed27ccf8d0307c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c28b309816a4d85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
