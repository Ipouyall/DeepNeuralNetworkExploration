{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install gensim -q"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec65989e04597f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import gensim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODE = \"local\" # valid modes are 'local', 'colab' and 'kaggle'\n",
    "SAMPLE_NUMBERS = 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a284f33993328c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if   MODE == \"local\":\n",
    "    BASE_PATH = \".\"\n",
    "elif MODE == \"colab\":\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Deep Neural Network - UT/CA 04\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(f\"Unknown mode {MODE}\")\n",
    "    exit(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "183e3ae190990fe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(BASE_PATH, \"twitter-suicidal_data.csv\")\n",
    "\n",
    "W2V_PATH = os.path.join(BASE_PATH, \"w2v_vectors.kv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1f939dd5898a0b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27110752c038c1d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b050729991fec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e92f34d72c6f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data statistics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194e8bd73f51d23b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa0e791280d469b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8728ed33e76432"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be081ae5bbcd06a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intentions = df[\"intention\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=intentions.index, y=intentions.values)\n",
    "plt.title(\"Intentions Distribution\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8c06e8ae4c8406"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57f7552c58ea498e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "nltk.download([\"stopwords\", \"punkt\", \"wordnet\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f16efd0184e90f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:          \n",
    "        return 'n'\n",
    "\n",
    "def preprocess_data(text: str):\n",
    "    \"\"\"\n",
    "    Preprocessing steps are as follows:\n",
    "    0. concatenation of the text (not necessary)\n",
    "    1. lowercase the text\n",
    "    2. remove punctuation\n",
    "    3. remove numbers\n",
    "    4. remove urls\n",
    "    5. remove usernames\n",
    "    6. remove extra spaces\n",
    "    7. convert emojis to text\n",
    "    8. remove non-word characters\n",
    "    9. lemmatization and tokenization of the text\n",
    "    10. remove stopwords\n",
    "    :param text: str\n",
    "    :return: tokens: list[str]\n",
    "    \"\"\"\n",
    "    # text = ''.join(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove urls,\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # remove usernames\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # convert emojis to text\n",
    "    text = convert_emoji_to_text(text)\n",
    "    # remove non-word characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # lemmatization and tokenization of the text\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    pos_tag = nltk.pos_tag(tokenized)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token, nltk_pos_tagger(tag))\n",
    "        for token, tag in pos_tag\n",
    "    ]\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe50f445aa09ecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_length_bound(text: str, length:int):\n",
    "    if len(text) <= length:\n",
    "        return text\n",
    "    mm = length // 2 - 3\n",
    "    rr = length - mm - 5\n",
    "    return text[:mm] + \"|...|\" + text[-rr:]\n",
    "    \n",
    "def draw_sample_processing(dataframe: pd.DataFrame, sample_numbers: int = SAMPLE_NUMBERS, show=True):\n",
    "    sample_text = dataframe.sample(sample_numbers)\n",
    "    label_rawT_procT = [\n",
    "        (label, tweet, preprocess_data(tweet)) \n",
    "        for tweet, label in zip(sample_text[\"tweet\"], sample_text[\"intention\"])\n",
    "    ]\n",
    "    if show:\n",
    "        for idx, (label, rawT, procT) in enumerate(label_rawT_procT, start=1):\n",
    "            procT = str(procT)\n",
    "            print(f\"Sample {idx}:\")\n",
    "            print(f\":::::::Label          : {label}\")\n",
    "            print(f\":::::::Raw Tweet      : {format_length_bound(rawT, 50):<50} ({len(rawT)})\")\n",
    "            print(f\":::::::Processed Tweet: {format_length_bound(procT, 50):<50} ({len(procT)})\")\n",
    "        return \n",
    "    return pd.DataFrame(label_rawT_procT, columns=[\"Intention\", \"Raw Tweet\", \"Processed Tweet\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ac7d394e03d9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_sample_processing(df, show=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a757363e6e6cbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a8f603c1fba10ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec - Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e129c7a31663741"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print available word2vec models\n",
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d15b31459fd1e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(W2V_PATH):\n",
    "    print(\"Loading Word2Vec model...\")\n",
    "    w2v_model = gensim.models.KeyedVectors.load(W2V_PATH, mmap='r')\n",
    "else:\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Saving Word2Vec model...\")\n",
    "    w2v_model.save(W2V_PATH)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bbe6faa1226b8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_VECTOR_DIM = w2v_model.vector_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d53926889defc38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataSet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21730f8dd01e5b4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Twitter(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, w2v_model: gensim.models.KeyedVectors, aggregate=True, **kw_args):\n",
    "        self.dataframe = dataframe\n",
    "        self.w2v_model = w2v_model\n",
    "        self.aggregate = aggregate\n",
    "        self.max_sequence_len = kw_args.get(\"sequence_len\", 512)\n",
    "        if not aggregate and \"sequence_len\" not in kw_args:\n",
    "            print(\"You Should provide 'sequence_len' to use not aggregate option!\")\n",
    "            exit(1)\n",
    "\n",
    "        self.len = len(self.dataframe)\n",
    "        self.vector_size = w2v_model.vector_size\n",
    "        self.lazy_mode = kw_args.get(\"lazy\", True)\n",
    "\n",
    "        self._proc_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.lazy_mode:\n",
    "            vector = self._get_word_vectors(self.dataframe.iloc[idx][\"tokens\"])\n",
    "            return vector, self.dataframe.iloc[idx][\"intention\"]\n",
    "        return self.dataframe.iloc[idx][\"vector\"], self.dataframe.iloc[idx][\"intention\"]\n",
    "\n",
    "    def get_vector_size(self):\n",
    "        return self.vector_size\n",
    "\n",
    "    def _proc_dataset(self):\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tweet\"].map(preprocess_data)\n",
    "        \n",
    "        # delete samples with empty tokens\n",
    "        lwz = len(self.dataframe)\n",
    "        self.dataframe = self.dataframe[self.dataframe[\"tokens\"].map(len) > 0]\n",
    "        self.dataframe.reset_index(drop=True, inplace=True)\n",
    "        print(f\"Deleted 0-Len Samples: {lwz - len(self.dataframe)}\")\n",
    "        self.seq_report()\n",
    "\n",
    "        if not self.aggregate:\n",
    "            self.dataframe[\"tokens\"] = self.dataframe[\"tokens\"].map(self._pad)\n",
    "\n",
    "        if not self.lazy_mode:\n",
    "            self.dataframe[\"vector\"] = self.dataframe[\"tokens\"].map(self._get_word_vectors)\n",
    "\n",
    "    def _get_word_vectors(self, tokens: list) -> torch.tensor:\n",
    "        if not self.aggregate:\n",
    "            return torch.stack([\n",
    "                torch.tensor(self.w2v_model[token] if token in self.w2v_model else np.zeros(self.vector_size)) for token in tokens\n",
    "            ])\n",
    "        wv = np.zeros(self.vector_size)\n",
    "        vc = 0\n",
    "        for token in tokens:\n",
    "            if token in self.w2v_model:\n",
    "                wv += self.w2v_model[token]\n",
    "                vc += 1\n",
    "        wv = torch.tensor(wv / max(vc, 1), dtype=torch.float32)\n",
    "        return wv\n",
    "\n",
    "    def _pad(self, tokens: list):\n",
    "        if len(tokens) >= self.max_sequence_len:\n",
    "            return tokens[:self.max_sequence_len]\n",
    "        return tokens + [\" \"] * (self.max_sequence_len - len(tokens))\n",
    "\n",
    "    def seq_report(self):\n",
    "        length_all = self.dataframe[\"tokens\"].map(len).tolist()\n",
    "        max_length = np.max(length_all)\n",
    "        print(f\"Sequence Length Report\")\n",
    "        print(f\":::::MAX  LENGTH:::[{max_length:^5}]\")\n",
    "        print(f\":::::MIN  LENGTH:::[{np.min(length_all):^5}]\")\n",
    "        print(f\":::::MEAN LENGTH:::[{np.mean(length_all):^5}]\")\n",
    "\n",
    "        all_tokens = set()\n",
    "        for token_set in self.dataframe[\"tokens\"].tolist():\n",
    "            all_tokens = all_tokens.union(set(token_set))\n",
    "        unique_tokens_count = len(all_tokens)\n",
    "        valid_tokens = sum(1 if token in self.w2v_model else 0 for token in all_tokens)\n",
    "        print(\"Sequence Tokenization Report\")\n",
    "        print(f\":::::All Unique Tokens:::[{unique_tokens_count:^6}\")\n",
    "        print(f\":::::All Valid Tokens:::[{valid_tokens:^6}\")\n",
    "        print(f\":::::Valid Tokens:::[{round(100*valid_tokens/unique_tokens_count, 2):^5}%]\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tensor(tokens: list):\n",
    "        return torch.tensor(tokens, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e3f4dc9d30a97f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b87e7e8b267a1c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Data into train-valid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd00ab880892e646"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"intention\"])\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a54facda2729872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "670fcd8db0e5651c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = Twitter(train_df, w2v_model)\n",
    "valid_dataset = Twitter(valid_df, w2v_model)\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset length: {len(valid_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e4a5b690afadad5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b1a37e4da1bd343"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd1da733e79676ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bed208d730342fc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DetectorLSTM(nn.Module):\n",
    "    def __init__(self, input_size, sequence_size, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super(DetectorLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5a723d9c509e7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-Layer LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f9b51f8fe11d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "38205ca41f01b782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN+2-Layer-LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1566b2a8428318c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ed27ccf8d0307c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c28b309816a4d85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
