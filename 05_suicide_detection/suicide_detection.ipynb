{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install gensim emoji nltk tqdm seaborn torch torchsummary -q"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec65989e04597f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf946d026ed2d98e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466c99ea9e39eb77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODE = \"local\" # valid modes are 'local', 'colab' and 'kaggle'\n",
    "SAMPLE_NUMBERS = 4\n",
    "TEST_PORTION = 0.2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a284f33993328c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e83a8b72b644c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 4e-4\n",
    "WEIGHT_DECAY = 1e-2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "SEQUENCE_LEN = 64\n",
    "CNN_FILTERS = 64\n",
    "LSTM_HIDDEN_DIM = 196\n",
    "LSTM_BIDIRECTIONAL = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d0826c5b8eb52c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auto setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9aa697ae043e65e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if   MODE == \"local\":\n",
    "    BASE_PATH = \".\"\n",
    "elif MODE == \"colab\":\n",
    "    BASE_PATH = \"/content/drive/MyDrive/Deep Neural Network - UT/CA 04\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(f\"Unknown mode {MODE}\")\n",
    "    exit(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "183e3ae190990fe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(BASE_PATH, \"twitter-suicidal_data.csv\")\n",
    "\n",
    "W2V_PATH = os.path.join(BASE_PATH, \"w2v_vectors.kv\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1f939dd5898a0b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27110752c038c1d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b050729991fec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e92f34d72c6f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data statistics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194e8bd73f51d23b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa0e791280d469b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8728ed33e76432"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be081ae5bbcd06a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intentions = df[\"intention\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=intentions.index, y=intentions.values)\n",
    "plt.title(\"Intentions Distribution\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8c06e8ae4c8406"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "labels = df[\"intention\"]\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "weights = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "print(class_weights)\n",
    "del labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9323acf7f7de0ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57f7552c58ea498e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "nltk.download([\"stopwords\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f16efd0184e90f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:          \n",
    "        return 'n'\n",
    "\n",
    "def preprocess_data(text: str):\n",
    "    \"\"\"\n",
    "    Preprocessing steps are as follows:\n",
    "    0. concatenation of the text (not necessary)\n",
    "    1. lowercase the text\n",
    "    2. remove punctuation\n",
    "    3. remove numbers\n",
    "    4. remove urls\n",
    "    5. remove usernames\n",
    "    6. remove extra spaces\n",
    "    7. convert emojis to text\n",
    "    8. remove non-word characters\n",
    "    9. lemmatization and tokenization of the text\n",
    "    10. remove stopwords\n",
    "    :param text: str\n",
    "    :return: tokens: list[str]\n",
    "    \"\"\"\n",
    "    # text = ''.join(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove urls,\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # remove usernames\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # convert emojis to text\n",
    "    text = convert_emoji_to_text(text)\n",
    "    # remove non-word characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # lemmatization and tokenization of the text\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    pos_tag = nltk.pos_tag(tokenized)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token, nltk_pos_tagger(tag))\n",
    "        for token, tag in pos_tag\n",
    "    ]\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe50f445aa09ecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_length_bound(text: str, length:int):\n",
    "    if len(text) <= length:\n",
    "        return text\n",
    "    mm = length // 2 - 3\n",
    "    rr = length - mm - 5\n",
    "    return text[:mm] + \"|...|\" + text[-rr:]\n",
    "    \n",
    "def draw_sample_processing(dataframe: pd.DataFrame, sample_numbers: int = SAMPLE_NUMBERS, show=True):\n",
    "    sample_text = dataframe.sample(sample_numbers)\n",
    "    label_rawT_procT = [\n",
    "        (label, tweet, preprocess_data(tweet)) \n",
    "        for tweet, label in zip(sample_text[\"tweet\"], sample_text[\"intention\"])\n",
    "    ]\n",
    "    if show:\n",
    "        for idx, (label, rawT, procT) in enumerate(label_rawT_procT, start=1):\n",
    "            procT = str(procT)\n",
    "            print(f\"Sample {idx}:\")\n",
    "            print(f\":::::::Label          : {label}\")\n",
    "            print(f\":::::::Raw Tweet      : {format_length_bound(rawT, 50):<50} ({len(rawT)})\")\n",
    "            print(f\":::::::Processed Tweet: {format_length_bound(procT, 50):<50} ({len(procT)})\")\n",
    "        return \n",
    "    return pd.DataFrame(label_rawT_procT, columns=[\"Intention\", \"Raw Tweet\", \"Processed Tweet\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ac7d394e03d9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_sample_processing(df, show=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a757363e6e6cbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec - Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e129c7a31663741"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print available word2vec models\n",
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d15b31459fd1e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(W2V_PATH):\n",
    "    print(\"Loading Word2Vec model...\")\n",
    "    w2v_model = gensim.models.KeyedVectors.load(W2V_PATH, mmap='r')\n",
    "else:\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Saving Word2Vec model...\")\n",
    "    w2v_model.save(W2V_PATH)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bbe6faa1226b8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_VECTOR_DIM = w2v_model.vector_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d53926889defc38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21730f8dd01e5b4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Twitter(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, w2v_model: gensim.models.KeyedVectors, aggregate=True, **kw_args):\n",
    "        self.dataframe = dataframe\n",
    "        self.w2v_model = w2v_model\n",
    "        self.aggregate = aggregate\n",
    "        self.max_sequence_len = kw_args.get(\"sequence_len\", 64)\n",
    "        if not aggregate and \"sequence_len\" not in kw_args:\n",
    "            print(\"You Should provide 'sequence_len' to use not aggregate option!\")\n",
    "            print(f\":::: Continue with sequence_len = {self.max_sequence_len} !\")\n",
    "\n",
    "        self.vector_size = w2v_model.vector_size\n",
    "        self.lazy_mode = kw_args.get(\"lazy\", True)\n",
    "\n",
    "        self._proc_dataset()\n",
    "\n",
    "        self.len = len(self.dataframe)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.lazy_mode:\n",
    "            vector = self._get_word_vectors(self.dataframe.iloc[idx][\"tokens\"])\n",
    "            return vector, self.dataframe.iloc[idx][\"intention\"]\n",
    "        return self.dataframe.iloc[idx][\"vector\"], self.dataframe.iloc[idx][\"intention\"]\n",
    "\n",
    "    def get_vector_size(self):\n",
    "        return self.vector_size\n",
    "\n",
    "    def _proc_dataset(self):\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tweet\"].map(preprocess_data)\n",
    "        \n",
    "        # delete samples with empty tokens\n",
    "        lwz = len(self.dataframe)\n",
    "        self.dataframe = self.dataframe[self.dataframe[\"tokens\"].map(len) > 0]\n",
    "        self.dataframe.reset_index(drop=True, inplace=True)\n",
    "        print(f\"Deleted 0-Len Samples: {lwz - len(self.dataframe)}\")\n",
    "        # self.seq_report()\n",
    "\n",
    "        if not self.aggregate:\n",
    "            self.dataframe[\"tokens\"] = self.dataframe[\"tokens\"].map(self._pad)\n",
    "\n",
    "        if not self.lazy_mode:\n",
    "            self.dataframe[\"vector\"] = self.dataframe[\"tokens\"].map(self._get_word_vectors)\n",
    "\n",
    "    def _get_word_vectors(self, tokens: list) -> torch.tensor:\n",
    "        if not self.aggregate:\n",
    "            return torch.stack([\n",
    "                torch.tensor(self.w2v_model[token] if token in self.w2v_model else np.zeros(self.vector_size), dtype=torch.float32)\n",
    "                for token in tokens\n",
    "            ])\n",
    "        wv = np.zeros(self.vector_size)\n",
    "        vc = 0\n",
    "        for token in tokens:\n",
    "            if token in self.w2v_model:\n",
    "                wv += self.w2v_model[token]\n",
    "                vc += 1\n",
    "        wv = torch.tensor(wv / max(vc, 1), dtype=torch.float32)\n",
    "        return wv\n",
    "\n",
    "    def _pad(self, tokens: list):\n",
    "        if len(tokens) >= self.max_sequence_len:\n",
    "            return tokens[:self.max_sequence_len]\n",
    "        return tokens + [\"<pad>\"] * (self.max_sequence_len - len(tokens))\n",
    "\n",
    "    def seq_report(self):\n",
    "        length_all = self.dataframe[\"tokens\"].map(len).tolist()\n",
    "        max_length = np.max(length_all)\n",
    "        print(f\"Sequence Length Report\")\n",
    "        print(f\":::::MAX  LENGTH:::[{max_length:^5}]\")\n",
    "        print(f\":::::MIN  LENGTH:::[{np.min(length_all):^5}]\")\n",
    "        print(f\":::::MEAN LENGTH:::[{np.mean(length_all):^5}]\")\n",
    "\n",
    "        all_tokens = set()\n",
    "        for token_set in self.dataframe[\"tokens\"].tolist():\n",
    "            all_tokens = all_tokens.union(set(token_set))\n",
    "        unique_tokens_count = len(all_tokens)\n",
    "        valid_tokens = sum(1 if token in self.w2v_model else 0 for token in all_tokens)\n",
    "        print(\"Sequence Tokenization Report\")\n",
    "        print(f\":::::All Unique Tokens:::[{unique_tokens_count:^6}\")\n",
    "        print(f\":::::All Valid Tokens:::[{valid_tokens:^6}\")\n",
    "        print(f\":::::Valid Tokens:::[{round(100*valid_tokens/unique_tokens_count, 2):^5}%]\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_tensor(tokens: list):\n",
    "        return torch.tensor(tokens, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e3f4dc9d30a97f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b87e7e8b267a1c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Data into train-valid"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd00ab880892e646"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=TEST_PORTION, random_state=42, stratify=df[\"intention\"])\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a54facda2729872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "670fcd8db0e5651c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = Twitter(train_df, w2v_model, aggregate=False, sequence_len=SEQUENCE_LEN)\n",
    "valid_dataset = Twitter(valid_df, w2v_model, aggregate=False, sequence_len=SEQUENCE_LEN)\n",
    "\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Valid dataset length: {len(valid_dataset)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e4a5b690afadad5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset.seq_report()\n",
    "print(\"*----------------------------------*\")\n",
    "valid_dataset.seq_report()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1a37e4da1bd343"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd1da733e79676ff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa29012e257e3d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_eval(model, loader, loss_function, device):\n",
    "    \"\"\"Returns test_loss, test_acc\"\"\"\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    total_samples = 0\n",
    "    if device == \"auto\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    model = model.to(device)\n",
    "    itr = tqdm(loader, total=len(loader), leave=False)\n",
    "\n",
    "    for inputs, labels in itr:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        \n",
    "        sample_count = len(inputs)\n",
    "    \n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_correct = torch.sum(preds == labels).item()\n",
    "        test_acc += test_correct\n",
    "\n",
    "        itr.set_description(\"(Eval)\")\n",
    "        itr.set_postfix(\n",
    "            loss=round(loss.item(), 5),\n",
    "            accuracy=round(test_correct/sample_count, 5),\n",
    "            \n",
    "            )\n",
    "        total_samples += sample_count\n",
    "\n",
    "    test_loss = test_loss / len(loader)\n",
    "    test_acc = test_acc / total_samples\n",
    "\n",
    "    return test_loss, test_acc\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5b33657ae501516"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        batch_size,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        train_set,\n",
    "        valid_set,\n",
    "        device,\n",
    "):\n",
    "    \n",
    "    if device == \"auto\":\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    itr = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_samples = 0\n",
    "        for idx, (inputs, labels) in enumerate(itr, start=1):\n",
    "            epoch_samples += len(inputs)\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            epoch_acc += torch.sum(preds == labels).item()\n",
    "        \n",
    "            loss = loss_function(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            itr.set_description(f\"(Training) Epoch [{epoch + 1}/{epochs}]\")\n",
    "            itr.set_postfix(\n",
    "              loss=round(loss.item(), 5),\n",
    "              accuracy=round(epoch_acc/epoch_samples, 5),\n",
    "              )\n",
    "        \n",
    "        epoch_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc/epoch_samples)\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss, valid_acc = model_eval(\n",
    "            model=model,\n",
    "            loader=valid_loader,\n",
    "            loss_function=loss_function,\n",
    "            device=device\n",
    "            )\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "    \n",
    "    history = {\n",
    "      \"train_loss\": train_losses,\n",
    "      \"train_acc\": train_accs,\n",
    "    \n",
    "      \"valid_loss\": valid_losses,\n",
    "      \"valid_acc\": valid_accs,\n",
    "    }\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bc1edee98f616b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def trend_plot_helper(pobj):\n",
    "    plt.figure(figsize=(5*len(pobj), 5))\n",
    "    for idx, (titler, plots) in enumerate(pobj.items(), start=1):\n",
    "        plt.subplot(1, len(pobj), idx)\n",
    "        for label, trend in plots:\n",
    "            plt.plot(range(1, len(trend)+1), trend, label=label)\n",
    "        yt, xt = titler.split(' - ')\n",
    "        plt.xlabel(xt)\n",
    "        plt.ylabel(yt)\n",
    "        plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e5acc20a7d5d446"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_confusion_matrix(model, dataset, device='auto'):\n",
    "    if device == 'auto':\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    probabilities = []\n",
    "    labels = []\n",
    "    \n",
    "    for data, label in tqdm(loader, leave=False, desc=\"Generate data\"):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        prob = model(data)\n",
    "        \n",
    "        probabilities.append(prob.detach())\n",
    "        labels.append(label.detach())\n",
    "    probabilities = torch.cat(probabilities, dim=0)\n",
    "    _, predicted = torch.max(probabilities.detach(), dim=1)\n",
    "    labels = torch.cat(labels, dim=0).detach().cpu().squeeze().numpy()\n",
    "    predicted = predicted.detach().cpu().squeeze().numpy()\n",
    "    \n",
    "    cm = metrics.confusion_matrix(\n",
    "        y_true=labels,\n",
    "        y_pred=predicted,\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(cm, cmap='Blues', annot=True, cbar=False, fmt=\".0f\",)\n",
    "    plt.xlabel('Predicted Label', labelpad=20)\n",
    "    plt.ylabel('True Label', labelpad=20)\n",
    "    plt.title('Confusion Matrix', fontsize=30)\n",
    "    \n",
    "    recall = metrics.recall_score(y_true=labels, y_pred=predicted, average='macro')\n",
    "    f1 = metrics.f1_score(y_true=labels, y_pred=predicted, average='macro')\n",
    "    precision = metrics.precision_score(y_true=labels, y_pred=predicted, average='macro')\n",
    "    report = metrics.classification_report(y_true=labels, y_pred=predicted)\n",
    "\n",
    "    return {'recall': recall, 'f1': f1, 'precision': precision, 'report': report}\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "819f88c36492de48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bed208d730342fc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DetectorLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1, bidirectional=False):\n",
    "        super(DetectorLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_bi_dim = 2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers>1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.hidden_bi_dim * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.hidden_bi_dim * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5a723d9c509e7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train modes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e073c31a727473a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train\n",
    "lstm_1l_model = DetectorLSTM(\n",
    "    input_size=EMBEDDING_VECTOR_DIM, \n",
    "    hidden_size=LSTM_HIDDEN_DIM, \n",
    "    bidirectional=LSTM_BIDIRECTIONAL,\n",
    "    num_layers=1, \n",
    "    num_classes=2,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82644f31350a2d35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_1l_model_train_history = train_model(\n",
    "    model=lstm_1l_model,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_function=nn.CrossEntropyLoss(weight=weights),\n",
    "    optimizer=torch.optim.Adam(lstm_1l_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    epochs=EPOCHS,\n",
    "    train_set=train_dataset,\n",
    "    valid_set=valid_dataset,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "205eeb10bcb61818"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", lstm_1l_model_train_history[\"train_acc\"]),\n",
    "            (\"Validation Acc\", lstm_1l_model_train_history[\"valid_acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", lstm_1l_model_train_history[\"train_loss\"]),\n",
    "            (\"Validation Loss\", lstm_1l_model_train_history[\"valid_loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f71ccd8933f362f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_1l_model_report = generate_confusion_matrix(\n",
    "    model=lstm_1l_model, \n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8df256aa2c7a982"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Recall:    {lstm_1l_model_report['recall']:.3f}\")\n",
    "print(f\"F1:        {lstm_1l_model_report['f1']:.3f}\")\n",
    "print(f\"Precision: {lstm_1l_model_report['precision']:.3f}\")\n",
    "print(lstm_1l_model_report['report'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69d280b2b3d16f98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del lstm_1l_model_train_history, lstm_1l_model, lstm_1l_model_report\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b723a2160d4c00c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-Layer LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26f9b51f8fe11d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_2l_model = DetectorLSTM(\n",
    "    input_size=EMBEDDING_VECTOR_DIM, \n",
    "    hidden_size=LSTM_HIDDEN_DIM,\n",
    "    bidirectional=LSTM_BIDIRECTIONAL,\n",
    "    num_layers=2, \n",
    "    num_classes=2\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38205ca41f01b782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_2l_model_train_history = train_model(\n",
    "    model=lstm_2l_model,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_function=nn.CrossEntropyLoss(weight=weights),\n",
    "    optimizer=torch.optim.Adam(lstm_2l_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    epochs=EPOCHS,\n",
    "    train_set=train_dataset,\n",
    "    valid_set=valid_dataset,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d12a8548a160858a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", lstm_2l_model_train_history[\"train_acc\"]),\n",
    "            (\"Validation Acc\", lstm_2l_model_train_history[\"valid_acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", lstm_2l_model_train_history[\"train_loss\"]),\n",
    "            (\"Validation Loss\", lstm_2l_model_train_history[\"valid_loss\"])\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "949e5c8c8632cc0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lstm_2l_model_report = generate_confusion_matrix(\n",
    "    model=lstm_2l_model, \n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "993d3a89f8b008b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Recall:    {lstm_2l_model_report['recall']:.3f}\")\n",
    "print(f\"F1:        {lstm_2l_model_report['f1']:.3f}\")\n",
    "print(f\"Precision: {lstm_2l_model_report['precision']:.3f}\")\n",
    "print(lstm_2l_model_report['report'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7efe99f137d9ddc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del lstm_2l_model_train_history, lstm_2l_model, lstm_2l_model_report\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4945fd541fe218c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN+2-Layer-LSTM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1566b2a8428318c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DetectorCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, filters, num_lstm_layers, bidirectional, num_classes, dropout=0.1, seq_len=64):\n",
    "        super(DetectorCNNLSTM, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=filters, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Unflatten(-1, (filters, seq_len//2)),\n",
    "            DetectorLSTM(\n",
    "                input_size=seq_len//2,\n",
    "                hidden_size=hidden_size,\n",
    "                bidirectional=bidirectional,\n",
    "                num_layers=num_lstm_layers,\n",
    "                num_classes=num_classes,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.permute(0, 2, 1)\n",
    "        out = self.network(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2ed27ccf8d0307c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_lstm_model = DetectorCNNLSTM(\n",
    "    input_size=EMBEDDING_VECTOR_DIM,\n",
    "    hidden_size=SEQUENCE_LEN,\n",
    "    filters=CNN_FILTERS,\n",
    "    bidirectional=LSTM_BIDIRECTIONAL,\n",
    "    num_lstm_layers=2,\n",
    "    num_classes=2,\n",
    "    dropout=0.5,\n",
    "    seq_len=SEQUENCE_LEN,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76d30ba6554b6c48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_lstm_model_train_history = train_model(\n",
    "    model=cnn_lstm_model,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    loss_function=nn.CrossEntropyLoss(weight=weights),\n",
    "    optimizer=torch.optim.Adam(cnn_lstm_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY),\n",
    "    epochs=EPOCHS,\n",
    "    train_set=train_dataset,\n",
    "    valid_set=valid_dataset,\n",
    "    device=DEVICE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a57ddcb0836683ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trend_plot_helper(\n",
    "    {\n",
    "        \"Accuracy - Epoch\": [\n",
    "            (\"Train Acc\", cnn_lstm_model_train_history[\"train_acc\"]),\n",
    "            (\"Validation Acc\", cnn_lstm_model_train_history[\"valid_acc\"]),\n",
    "        ],\n",
    "        \"Loss - Epoch\": [\n",
    "            (\"Train Loss\", cnn_lstm_model_train_history[\"train_loss\"]),\n",
    "            (\"Validation Loss\", cnn_lstm_model_train_history[\"valid_loss\"])\n",
    "        ]\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac375ced89f224ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cnn_lstm_model_report = generate_confusion_matrix(\n",
    "    model=cnn_lstm_model, \n",
    "    dataset=valid_dataset,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a09473d5783fa70f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Recall:    {cnn_lstm_model_report['recall']:.3f}\")\n",
    "print(f\"F1:        {cnn_lstm_model_report['f1']:.3f}\")\n",
    "print(f\"Precision: {cnn_lstm_model_report['precision']:.3f}\")\n",
    "print(cnn_lstm_model_report['report'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee01e4607601dd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del cnn_lstm_model_train_history, cnn_lstm_model, cnn_lstm_model_report\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31ce53febd6246e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c28b309816a4d85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
