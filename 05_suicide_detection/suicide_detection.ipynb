{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "!pip install gensim -q"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T14:59:16.030407Z",
     "start_time": "2023-12-18T14:59:14.450808Z"
    }
   },
   "id": "fec65989e04597f0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T15:08:48.100819Z",
     "start_time": "2023-12-18T15:08:46.748343Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import gensim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODE = \"local\" # valid modes are 'local', 'colab' and 'kaggle'\n",
    "SAMPLE_NUMBERS = 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a284f33993328c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if MODE == \"local\":\n",
    "    BASE_PATH = \".\"\n",
    "else:\n",
    "    print(f\"Unknown mode {MODE}\")\n",
    "    exit(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "183e3ae190990fe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(BASE_PATH, \"twitter-suicidal_data.csv\")\n",
    "\n",
    "W2V_PATH = os.path.join(BASE_PATH, \"w2v_vectors.kv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1f939dd5898a0b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27110752c038c1d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32b050729991fec9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e92f34d72c6f62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data statistics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194e8bd73f51d23b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa0e791280d469b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f8728ed33e76432"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Number of samples: {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be081ae5bbcd06a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intentions = df[\"intention\"].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x=intentions.index, y=intentions.values)\n",
    "plt.title(\"Intentions Distribution\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8c06e8ae4c8406"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57f7552c58ea498e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "nltk.download([\"stopwords\", \"punkt\", \"wordnet\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f16efd0184e90f40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text):\n",
    "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:          \n",
    "        return 'n'\n",
    "\n",
    "def preprocess_data(text: str):\n",
    "    \"\"\"\n",
    "    Preprocessing steps are as follows:\n",
    "    0. concatenation of the text (not necessary)\n",
    "    1. lowercase the text\n",
    "    2. remove punctuation\n",
    "    3. remove numbers\n",
    "    4. remove urls\n",
    "    5. remove usernames\n",
    "    6. remove extra spaces\n",
    "    7. convert emojis to text\n",
    "    8. remove non-word characters\n",
    "    9. lemmatization and tokenization of the text\n",
    "    10. remove stopwords\n",
    "    :param text: str\n",
    "    :return: tokens: list[str]\n",
    "    \"\"\"\n",
    "    # text = ''.join(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove urls,\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # remove usernames\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # convert emojis to text\n",
    "    text = convert_emoji_to_text(text)\n",
    "    # remove non-word characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # lemmatization and tokenization of the text\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    pos_tag = nltk.pos_tag(tokenized)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token, nltk_pos_tagger(tag))\n",
    "        for token, tag in pos_tag\n",
    "    ]\n",
    "    # remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    return tokens\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe50f445aa09ecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_length_bound(text: str, length:int):\n",
    "    if len(text) <= length:\n",
    "        return text\n",
    "    mm = length // 2 - 3\n",
    "    rr = length - mm - 5\n",
    "    return text[:mm] + \"|...|\" + text[-rr:]\n",
    "    \n",
    "def draw_sample_processing(dataframe: pd.DataFrame, sample_numbers: int = SAMPLE_NUMBERS, show=True):\n",
    "    sample_text = dataframe.sample(sample_numbers)\n",
    "    label_rawT_procT = [\n",
    "        (label, tweet, preprocess_data(tweet)) \n",
    "        for tweet, label in zip(sample_text[\"tweet\"], sample_text[\"intention\"])\n",
    "    ]\n",
    "    if show:\n",
    "        for idx, (label, rawT, procT) in enumerate(label_rawT_procT, start=1):\n",
    "            procT = str(procT)\n",
    "            print(f\"Sample {idx}:\")\n",
    "            print(f\":::::::Label          : {label}\")\n",
    "            print(f\":::::::Raw Tweet      : {format_length_bound(rawT, 50):<50} ({len(rawT)})\")\n",
    "            print(f\":::::::Processed Tweet: {format_length_bound(procT, 50):<50} ({len(procT)})\")\n",
    "        return \n",
    "    return pd.DataFrame(label_rawT_procT, columns=[\"Intention\", \"Raw Tweet\", \"Processed Tweet\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11ac7d394e03d9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_sample_processing(df, show=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a757363e6e6cbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a8f603c1fba10ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec - Word Embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e129c7a31663741"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# print available word2vec models\n",
    "import gensim.downloader as api\n",
    "print(list(api.info()['models'].keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T15:08:56.237303Z",
     "start_time": "2023-12-18T15:08:53.702882Z"
    }
   },
   "id": "8d15b31459fd1e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(W2V_PATH):\n",
    "    print(\"Loading Word2Vec model...\")\n",
    "    w2v_model = gensim.models.KeyedVectors.load(W2V_PATH, mmap='r')\n",
    "else:\n",
    "    print(\"Downloading Word2Vec model...\")\n",
    "    w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"Saving Word2Vec model...\")\n",
    "    w2v_model.save(W2V_PATH)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55bbe6faa1226b8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBEDDING_VECTOR_DIM = w2v_model.vector_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d53926889defc38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21730f8dd01e5b4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Twitter(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, w2v_model: gensim.models.KeyedVectors, max_length: int = 50):\n",
    "        self.dataframe = dataframe\n",
    "        self.w2v_model = w2v_model\n",
    "        self.max_length = max_length\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tweet\"].apply(preprocess_data)\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tokens\"].apply(self._get_word_vectors)\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tokens\"].apply(self._pad)\n",
    "        self.dataframe[\"tokens\"] = self.dataframe[\"tokens\"].apply(self._to_tensor)\n",
    "        \n",
    "        self.len = len(self.dataframe)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx][\"tokens\"], self.dataframe.iloc[idx][\"intention\"]\n",
    "    \n",
    "    def _get_word_vectors(self, tokens: list):\n",
    "        return [self.w2v_model[token] for token in tokens if token in self.w2v_model]\n",
    "    \n",
    "    def _pad(self, tokens: list):\n",
    "        if len(tokens) >= self.max_length:\n",
    "            return tokens[:self.max_length]\n",
    "        return tokens + [np.zeros(EMBEDDING_VECTOR_DIM)] * (self.max_length - len(tokens))\n",
    "    \n",
    "    def _to_tensor(self, tokens: list):\n",
    "        return torch.tensor(tokens, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e3f4dc9d30a97f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
